# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10CEoIkU7eBJWJVJ-c9Nm3ycJCvk5Noi2
"""

# Install necessary libraries
# !pip install groq google-api-python-client requests feedparser

import requests
import groq
import json
import urllib.parse
import feedparser
from googleapiclient.discovery import build

# Step 1: Set API Keys (Replace with your actual keys)
GROQ_API_KEY = ""
YOUTUBE_API_KEY = ""

# Step 2: Initialize Groq Client
groq_client = groq.Groq(api_key=GROQ_API_KEY)

# List of EdTech platforms to exclude
BLACKLISTED_DOMAINS = ["coursera.org", "udemy.com", "edx.org", "skillshare.com", "khanacademy.org"]

# Function to filter out competing links
def filter_links(links):
    return [link for link in links if not any(domain in link for domain in BLACKLISTED_DOMAINS)]

# Step 3: Function to Get AI Summary Using Groq
def get_ai_summary(topic):
    response = groq_client.chat.completions.create(
        model="mixtral-8x7b-32768",
        messages=[
            {"role": "system", "content": "You are a research assistant."},
            {"role": "user", "content": f"Provide a summary of {topic}."}
        ]
    )
    return response.choices[0].message.content

# Step 4: Function to Fetch Research Papers from arXiv
def fetch_latest_research_papers(query):
    # Encode query to handle spaces and special characters
    encoded_query = urllib.parse.quote(query)
    url = f"http://export.arxiv.org/api/query?search_query=all:{encoded_query}&start=0&max_results=3"
    feed = feedparser.parse(url)

    research_papers = "===== Research Papers =====\n\n"
    filtered_papers = []

    for entry in feed.entries:
        title = entry.title
        summary = entry.summary
        authors = ", ".join(author.name for author in entry.authors)
        link = entry.link

        if not any(domain in link for domain in BLACKLISTED_DOMAINS):  # Exclude blacklisted domains
            filtered_papers.append(
                f"Title: {title}\nAuthors: {authors}\nSummary: {summary[:300]}...\nRead More: {link}\n\n"
            )

    return research_papers + "\n".join(filtered_papers) if filtered_papers else "No research papers found."

# Step 5: Function to Fetch YouTube Videos (Filtering EdTech Competitors)
def get_youtube_videos(query):
    youtube = build("youtube", "v3", developerKey=YOUTUBE_API_KEY)
    request = youtube.search().list(
        q=query,
        part="id,snippet",
        maxResults=5
    )
    response = request.execute()

    videos = []
    for item in response.get("items", []):
        if "videoId" in item.get("id", {}):
            video_title = item["snippet"]["title"]
            video_url = f"https://www.youtube.com/watch?v={item['id']['videoId']}"

            # Exclude competing EdTech platforms based on title keywords
            if not any(edtech in video_title.lower() for edtech in ["coursera", "udemy", "edx", "skillshare", "khan academy", "simplilearn", "geeksforgeeks", "simplilearn"]):
                videos.append(f"{video_title}: {video_url}")

    return videos if videos else ["No relevant videos found."]

# Step 6: Function to Process User Query
def get_resources(topic):
    print("Fetching AI-generated summary from Groq...")
    ai_summary = get_ai_summary(topic)

    print("\nFetching latest research papers...")
    research_papers = fetch_latest_research_papers(topic)

    print("\nFetching relevant YouTube videos...")
    youtube_videos = get_youtube_videos(topic)

    # Display Results
    print("\n===== AI Summary =====")
    print(ai_summary)

    print("\n===== Research Papers =====")
    print(research_papers)

    print("\n===== YouTube Videos =====")
    for video in youtube_videos:
        print(video)

# Step 7: Ask User for Input & Get Resources
topic = input("Enter a topic: ")
get_resources(topic)

